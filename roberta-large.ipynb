{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RoBERTa-large\n",
    "https://huggingface.co/FacebookAI/xlm-roberta-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Mar  2 16:33:04 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.216.03             Driver Version: 535.216.03   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          On  | 00000000:02:00.0 Off |                   On |\n",
      "| N/A   34C    P0             101W / 300W |  12080MiB / 81920MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "\n",
      "+---------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                          |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                |        ECC|                       |\n",
      "|==================+================================+===========+=======================|\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|  0    7   0   0  |              12MiB /  9728MiB  | 14      0 |  1   0    0    0    0 |\n",
      "|                  |               0MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (4.49.0)\n",
      "Requirement already satisfied: filelock in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from transformers) (0.29.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from transformers) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from transformers) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from requests->transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: datasets in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (3.3.2)\n",
      "Requirement already satisfied: filelock in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from datasets) (19.0.1)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from datasets) (3.11.13)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from datasets) (0.29.1)\n",
      "Requirement already satisfied: packaging in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.6)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from aiohttp->datasets) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from aiohttp->datasets) (1.18.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: torch in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: filelock in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (3.17.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.2.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (3.2.0)\n",
      "Requirement already satisfied: setuptools in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (75.8.2)\n",
      "Requirement already satisfied: sympy==1.13.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from scikit-learn) (2.2.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from scikit-learn) (1.15.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from scikit-learn) (3.5.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from pandas) (2.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from pandas) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: seaborn in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: pandas>=1.2 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from seaborn) (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from pandas>=1.2->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: matplotlib in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib) (4.56.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib) (2.2.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib) (11.1.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib) (3.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /vol/venv/rk1121/nlp/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers \n",
    "%pip install datasets \n",
    "%pip install torch \n",
    "%pip install scikit-learn\n",
    "%pip install pandas\n",
    "%pip install seaborn\n",
    "%pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# prepare logger\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "transformers_logger = logging.getLogger(\"transformers\")\n",
    "transformers_logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/venv/rk1121/nlp/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.optim as optim\n",
    "import ast\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import Dict, Any, List\n",
    "from datetime import datetime\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load RoBERTa-large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer and model\n",
    "MODEL_NAME = \"roberta-large\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = RobertaForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=2) \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "raw_dir = \"models/raw\"\n",
    "model.save_pretrained(raw_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load raw text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary(label: Any) -> int:\n",
    "    \"\"\"\n",
    "    Paragraphs with original labels of 0 or 1 are considered to be negative examples of PCL and will have the label 0 = negative.\n",
    "\t\tParagraphs with original labels of 2, 3 or 4 are considered to be positive examples of PCL and will have the label 1 = positive.\n",
    "    \"\"\"\n",
    "    # If label_array is a string, convert it to a list\n",
    "    return 0 if label == 0 or label == 1 else 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text Data shape: (10469, 7)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_DATA_PATH = \"data/dontpatronizeme_pcl.tsv\"\n",
    "\n",
    "column_names = [\"par_id\", \"art_id\", \"keyword\", \"country\", \"text\", \"orig_label\"]\n",
    "text_data = pd.read_csv(TRAIN_DATA_PATH, sep=\"\\t\", header=None, names=column_names)\n",
    "text_data[\"label\"] = text_data[\"orig_label\"].apply(convert_to_binary)\n",
    "\n",
    "print(\"Text Data shape:\", text_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Pre-)Tokenize Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text, tokenizer, max_length=128):\n",
    "  return tokenizer(\n",
    "    str(text),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_length,\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    return_tensors=\"pt\"\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretokenized_data = []\n",
    "for index, row in text_data.iterrows():\n",
    "  tokenized = tokenize(row[\"text\"], tokenizer)\n",
    "  tokenized = { key : value.squeeze(0).clone() for key, value in tokenized.items() }\n",
    "  tokenized[\"labels\"] = row[\"label\"]\n",
    "  pretokenized_data.append(tokenized)\n",
    "\n",
    "with open(\"pretokenized.pkl\", \"wb\") as f:\n",
    "  pickle.dump(pretokenized_data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load into DataLoader's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DontPatronizeMeDataset(Dataset):\n",
    "  def __init__(self, data, tokenizer = None, max_length=128):\n",
    "    self.tokenizer = tokenizer\n",
    "\n",
    "    if self.tokenizer is not None:\n",
    "      self.max_length = max_length\n",
    "      self.data = data.reset_index(drop = True)\n",
    "    else:\n",
    "      self.data = data\n",
    "\n",
    "  def __len__(self) -> int:\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "    if self.tokenizer is not None:\n",
    "      text = str(self.data.loc[idx, \"text\"])\n",
    "      label = self.data.loc[idx, \"label\"]\n",
    "\n",
    "      encoding = self.tokenizer(\n",
    "        text, \n",
    "        add_special_tokens=True,\n",
    "        max_length=self.max_length,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_tensors=\"pt\"\n",
    "      )\n",
    "\n",
    "      encoding = { key: value.squeeze(0) for key, value in encoding.items() }\n",
    "      encoding[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "      return encoding\n",
    "    else:\n",
    "      return self.data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"pretokenized.pkl\", \"rb\") as f:\n",
    "  pretokenized_data = pickle.load(f)\n",
    "\n",
    "labels = [int(item[\"labels\"]) for item in pretokenized_data]\n",
    "\n",
    "train_data, test_data = train_test_split(\n",
    "  pretokenized_data,\n",
    "  test_size = 0.2,\n",
    "  random_state = 16,\n",
    "  stratify=labels\n",
    ")\n",
    "\n",
    "train_dataset = DontPatronizeMeDataset(train_data)\n",
    "test_dataset = DontPatronizeMeDataset(test_data)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See if there is class imabalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Train ---\n",
      "{0: 7581, 1: 794}\n",
      "{0: '90.52%', 1: '9.48%'}\n",
      "--- Test ---\n",
      "{0: 1895, 1: 199}\n",
      "{0: '90.50%', 1: '9.50%'}\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "train_labels = [int(item[\"labels\"]) for item in train_data]\n",
    "test_labels = [int(item[\"labels\"]) for item in test_data]\n",
    "\n",
    "train_counts = Counter(train_labels)\n",
    "test_counts = Counter(test_labels)\n",
    "\n",
    "train_sorted = { k : v for k, v in sorted(train_counts.items()) }\n",
    "test_sorted = { k : v for k, v in sorted(test_counts.items()) }\n",
    "\n",
    "print(\"--- Train ---\")\n",
    "print(train_sorted)\n",
    "train_total = sum(train_counts.values())\n",
    "print({ k: f\"{(100 * v / train_total):.2f}%\" for k, v in train_sorted.items() })\n",
    "\n",
    "print(\"--- Test ---\")\n",
    "print(test_sorted)\n",
    "test_total = sum(test_counts.values())\n",
    "print({ k: f\"{(100 * v / test_total):.2f}%\" for k, v in test_sorted.items() })\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 5))\n",
    "\n",
    "axes[0].bar(list(train_sorted.keys()), list(train_sorted.values()), color='skyblue')\n",
    "axes[0].set_title('Train Data Label Distribution')\n",
    "axes[0].set_xlabel('Label')\n",
    "axes[0].set_ylabel('Count')\n",
    "\n",
    "axes[1].bar(list(test_sorted.keys()), list(test_sorted.values()), color='salmon')\n",
    "axes[1].set_title('Test Data Label Distribution')\n",
    "axes[1].set_xlabel('Label')\n",
    "axes[1].set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, loader: DataLoader, save_path: str = None):\n",
    "  model.eval()\n",
    "\n",
    "  predictions = []\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for batch in loader:\n",
    "      inputs = batch[\"input_ids\"].to(device)\n",
    "      attention_mask = batch[\"attention_mask\"].to(device)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "\n",
    "      outputs = model(inputs, attention_mask=attention_mask)\n",
    "      logits = outputs.logits\n",
    "\n",
    "      preds = torch.argmax(logits, dim=1)\n",
    "      predictions.extend(preds.cpu().numpy().tolist())\n",
    "  \n",
    "  if save_path is not None:\n",
    "    with open(save_path, \"w\") as f:\n",
    "      for prediction in predictions:\n",
    "        f.write(f\"{prediction}\\n\")\n",
    "\n",
    "  return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader: DataLoader):\n",
    "  y_true = []\n",
    "  y_pred = []\n",
    "\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "      for batch in loader:\n",
    "          inputs = batch[\"input_ids\"].to(device)\n",
    "          attention_mask = batch[\"attention_mask\"].to(device)\n",
    "          labels = batch[\"labels\"].to(device)\n",
    "\n",
    "          outputs = model(inputs, attention_mask=attention_mask)\n",
    "          logits = outputs.logits\n",
    "\n",
    "          predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "          y_true.extend(labels.cpu().numpy().tolist())\n",
    "          y_pred.extend(predictions.cpu().numpy().tolist())\n",
    "\n",
    "  # # Convert to numpy arrays\n",
    "  y_true = np.array(y_true)\n",
    "  y_pred = np.array(y_pred)\n",
    "\n",
    "  accuracy = accuracy_score(y_true, y_pred)\n",
    "  f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "  # predictions, labels = predict(model, loader=loader, with_labels=True, save_path=save_path)\n",
    "\n",
    "  # Compute metrics\n",
    "  # accuracy = accuracy_score(labels, predictions)\n",
    "  # f1 = f1_score(labels, predictions, average=\"macro\")\n",
    "\n",
    "  return accuracy, f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.095033\n",
      "Test F1 Score: 0.086786\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(raw_dir).to(device)\n",
    "\n",
    "# approx 1min\n",
    "test_acc, test_f1 = evaluate(model, test_loader)\n",
    "\n",
    "# Acc: 0.101242\n",
    "# F1:  0.094094\n",
    "print(f\"Test Accuracy: {test_acc:.6f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tune the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.amp import autocast, GradScaler\n",
    "\n",
    "def finetune(\n",
    "  model, \n",
    "  loader: DataLoader, \n",
    "  num_epochs: int = 10, \n",
    "  lr: float = 2e-5, \n",
    "  patience: int = 2\n",
    "):\n",
    "\n",
    "  best_val_loss = float(\"inf\")\n",
    "  epochs_no_improv = 0\n",
    "\n",
    "  optimiser = optim.AdamW(model.parameters(), lr=lr)\n",
    "  total_steps = len(loader) * num_epochs\n",
    "  scheduler = get_linear_schedule_with_warmup(\n",
    "    optimiser, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=total_steps\n",
    "  )\n",
    "\n",
    "  scaler = GradScaler(device=\"cuda\")\n",
    "\n",
    "  timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "  print(f\"{timestamp} - num_epochs = {num_epochs} | lr = {lr} | patience = {patience}\")\n",
    "\n",
    "  for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss: float = 0.0\n",
    "\n",
    "    for batch in loader:\n",
    "      optimiser.zero_grad()\n",
    "\n",
    "      inputs = batch[\"input_ids\"].to(device)\n",
    "      attention_mask = batch[\"attention_mask\"].to(device)\n",
    "      labels = batch[\"labels\"].to(device)\n",
    "\n",
    "      # Use `autocast` for mixed precision forward pass\n",
    "      with autocast(device_type=\"cuda\"):\n",
    "        outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "\n",
    "      scaler.scale(loss).backward()\n",
    "      scaler.step(optimiser)\n",
    "      scheduler.step()\n",
    "      scaler.update()\n",
    "\n",
    "      total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "\n",
    "    model.eval()\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "      for batch in loader:\n",
    "        inputs = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"labels\"].to(device)\n",
    "\n",
    "        with autocast(device_type=\"cuda\"):\n",
    "          outputs = model(inputs, attention_mask=attention_mask, labels=labels)\n",
    "          loss = outputs.loss\n",
    "        total_val_loss += loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(loader)\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "    print(f\"{timestamp} - Epoch {epoch + 1}/{num_epochs} | Train Loss: {avg_loss:.6f} | Validation Loss: {avg_val_loss:.6f}\")\n",
    "\n",
    "    if avg_val_loss < best_val_loss:\n",
    "      best_val_loss = avg_val_loss\n",
    "      epochs_no_improv = 0\n",
    "      best_model_state = model.state_dict()\n",
    "\n",
    "    else:\n",
    "      epochs_no_improv += 1\n",
    "      if epochs_no_improv >= patience:\n",
    "        print(f\"Stopping at epoch {epoch}...\")\n",
    "        break\n",
    "    \n",
    "  model.load_state_dict(best_model_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:27:15 - num_epochs = 10 | lr = 2e-05 | patience = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/venv/rk1121/nlp/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:227: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:31:18 - Epoch 1/10 | Train Loss: 0.307378 | Validation Loss: 0.215937\n",
      "17:35:23 - Epoch 2/10 | Train Loss: 0.209410 | Validation Loss: 0.122734\n",
      "17:39:28 - Epoch 3/10 | Train Loss: 0.141320 | Validation Loss: 0.069844\n",
      "17:43:32 - Epoch 4/10 | Train Loss: 0.071849 | Validation Loss: 0.014653\n",
      "17:47:37 - Epoch 5/10 | Train Loss: 0.032062 | Validation Loss: 0.010461\n",
      "17:51:42 - Epoch 6/10 | Train Loss: 0.019765 | Validation Loss: 0.005185\n",
      "17:55:47 - Epoch 7/10 | Train Loss: 0.010363 | Validation Loss: 0.002446\n",
      "17:59:52 - Epoch 8/10 | Train Loss: 0.004966 | Validation Loss: 0.000759\n",
      "18:03:57 - Epoch 9/10 | Train Loss: 0.002219 | Validation Loss: 0.000318\n",
      "18:08:02 - Epoch 10/10 | Train Loss: 0.001054 | Validation Loss: 0.000296\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "\n",
    "model = RobertaForSequenceClassification.from_pretrained(raw_dir).to(device)\n",
    "\n",
    "# approx 5min per epoch\n",
    "finetune(model, train_loader, num_epochs = 10)\n",
    "\n",
    "basic_finetuned_dir = \"models/basic_finetuned\"\n",
    "model.save_pretrained(basic_finetuned_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.923591\n",
      "Test F1 Score: 0.754404\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(basic_finetuned_dir).to(device)\n",
    "\n",
    "# approx 1min\n",
    "test_acc, test_f1 = evaluate(model, test_loader)\n",
    "\n",
    "# Acc: 0.923591\n",
    "# F1:  0.754404\n",
    "print(f\"Test Accuracy: {test_acc:.6f}\")\n",
    "print(f\"Test F1 Score: {test_f1:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Data shape: (3832, 5)\n"
     ]
    }
   ],
   "source": [
    "TEST_DATA_PATH = \"data/task4_test.tsv\"\n",
    "\n",
    "column_names = [\"par_id\", \"art_id\", \"keyword\", \"country\", \"text\"]\n",
    "test_data = pd.read_csv(TEST_DATA_PATH, sep=\"\\t\", header=None, names=column_names)\n",
    "\n",
    "print(\"Test Data shape:\", test_data.shape)\n",
    "\n",
    "# print(test_text_data.head())\n",
    "\n",
    "test_dataset = DontPatronizeMeDataset(test_data, tokenizer, max_length=128)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dev_dataset = DontPatronizeMeDataset(text_data, tokenizer, max_length=128)\n",
    "full_dev_loader = DataLoader(full_dev_dataset, batch_size=16, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating predictions for Dev...\n",
      "Generating predictions for Test...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 1,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " 0,\n",
       " ...]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RobertaForSequenceClassification.from_pretrained(basic_finetuned_dir).to(device)\n",
    "\n",
    "print(\"Generating predictions for Dev...\")\n",
    "predict(model, full_dev_loader, save_path=\"dev.txt\")\n",
    "\n",
    "print(\"Generating predictions for Test...\")\n",
    "predict(model, test_loader, save_path=\"test.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
